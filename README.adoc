= [Draft] kubernetes-vault
Vault 0.8.2 on top of Kubernetes (backed by etcd 3.1.10).
:icons: font
:toc:

== Pre-Requisites

* A working GKE cluster running Kubernetes v1.7.5 with legacy authorization
  *_disabled_*.
* A working installation of `gcloud` configured to access the target Google
  Cloud Platform project.
* A working installation of `kubectl` configured to access the target cluster.
* A working installation of https://github.com/cloudflare/cfssl[`cfssl`]
  (more details <<#bookmark-cfssl, below>>).
* The https://https://www.vaultproject.io/[Vault] binary (more
  details <<#bookmark-vault-binary, below>>).

== Before proceeding

[IMPORTANT]
====
Disabling legacy authorization means that _Role-Based Access Control_
(https://kubernetes.io/docs/admin/authorization/rbac/[RBAC]) is enabled. RBAC
is the new way to manage permissions in Kubernetes 1.6+. Reading the RBAC
documentation and understanding the differences between RBAC and legacy
authorization is strongly encouraged.
====

[IMPORTANT]
====
One must grant themselves the `cluster-admin` role *manually* before
proceeding. This is due to a known issue with RBAC on GKE. To do that one must
retrieve their identity using `gcloud` by running

[source,bash]
----
$ MY_GCLOUD_USER=$(gcloud info \
  | grep Account \
  | awk -F'[][]' '{print $2}')
----

and then create a `ClusterRoleBinding` by running

[source,bash]
----
$ kubectl create clusterrolebinding \
  <cluster-role-binding-name> \
  --clusterrole=cluster-admin \
  --user=${MY_GCLOUD_USER}
----

One must replace `<cluster-role-binding-name>` in the above command with a
unique, meaningful name. It is a good idea to include one's identity in the
name (e.g., `j.doe-cluster-admin`).
====

[[bookmark-cfssl]]
[IMPORTANT]
====
`cfssl` can be installed either

* from https://github.com/cloudflare/cfssl[source] — this requires a Go 1.6+
  environment setup on one's workstation; or
* by downloading the https://pkg.cfssl.org/[binaries] for one's architecture
  and dropping them on `${PATH}`.

If downloading the binaries one needs only to download `cfssl` and `cfssljson`.
====

[IMPORTANT]
====
Vault can be installed by downloading the
https://www.vaultproject.io/downloads.html[binary] for one's architecture and
dropping it on `${PATH}`. The binary can act both as server and client, and
will be needed later on to configure the Vault deployment.
====

[IMPORTANT]
====
All commands should be run in a terminal and from the root of the repository.
====

== Creating the `etcd` and `vault` namespaces

To better group and manage the major components of the solution it is
recommended to create two namespaces in the cluster — one for etcd-related
resources and another one for Vault-related ones:

[source,bash]
----
$ kubectl create namespace etcd
namespace "etcd" created
----

[source,bash]
----
$ kubectl create namespace vault
namespace "vault" created
----

[IMPORTANT]
====
If one does not follow this approach, or if one chooses different names for the
namespaces, one must update the scripts and Kubernetes descriptors in this
repository accordingly.
====

== Deploying etcd

=== Deploying `etcd-operator`

`etcd-operator` will be responsible for managing the etcd cluster that Vault
will use as storage backend. It will handle tasks such as periodic backups and
member recovery in disaster scenarios. `etcd-operator` and the cluster itself
will live in the `etcd` namespace.

To start with, and since RBAC is active on the cluster, one needs to setup
adequate permissions. To do this one needs to

* create a `ClusterRole` specifying a list of permissions;
* create a dedicated `ServiceAccount` for `etcd-operator`;
* create a `CluserRoleBinding` that grants these permissions to the service
 account.

This is made by running the following commands:

[source,bash]
----
$ kubectl create -f ./etcd-operator/etcd-operator-clusterrole.yaml
clusterrole "etcd-operator" created
----

[source,bash]
----
$ kubectl create -f ./etcd-operator/etcd-operator-serviceaccount.yaml
serviceaccount "etcd-operator" created
----

[source,bash]
----
$ kubectl create -f ./etcd-operator/etcd-operator-clusterrolebinding.yaml
clusterrolebinding "etcd-operator" created
----

One is now ready to deploy `etcd-operator` itself:

[source,bash]
----
$ kubectl create -f ./etcd-operator/etcd-operator-deployment.yaml
deployment "etcd-operator" created
----

[TIP]
====
At this point it is a good idea to check whether the deployment succeeded. One
should wait a couple of minutes and then run:

[source,bash]
----
$ ETCD_OPERATOR_POD_NAME=$(kubectl get pod --namespace etcd \
  | grep etcd-operator \
  | awk '{print $1}')
----

[source,bash]
----
$ kubectl logs --follow --namespace etcd "${ETCD_OPERATOR_POD_NAME}"
time="2017-09-14T12:20:02Z" level=info msg="etcd-operator Version: 0.5.2"
time="2017-09-14T12:20:02Z" level=info msg="Git SHA: 0fa2f7d"
time="2017-09-14T12:20:02Z" level=info msg="Go Version: go1.8.1"
time="2017-09-14T12:20:02Z" level=info msg="Go OS/Arch: linux/amd64"
time="2017-09-14T12:20:02Z" level=info msg="Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"etcd", Name:"etcd-operator", UID:"08f3a0d2-9947-11e7-809e-42010a9a00c3", APIVersion:"v1", ResourceVersion:"870", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' etcd-operator-3160827283-4ksk8 became leader
----

If the output doesn't differ much from the example, and in particular if one
doesn't see any error messages, it is safe to proceed.
====

=== Generating TLS certificates for etcd

One is now almost ready to create the etcd cluster that will back the Vault
deployment. However, before proceeding, one needs to generate TLS certificates
to secure communications with the etcd cluster.

[NOTE]
====
Even though the etcd cluster won't be exposed to outside the Kubernetes
cluster, and even though Vault encrypts all data before it reaches the
network, it is always a good security measure to enable TLS communication in
the cluster.
====

One will need different types of certificates for establishing TLS:

* A server certificate which etcd will use for serving _client-to-server_
 requests (such as a request for a key).
* A server certificate which etcd will use for serving _server-to-server_
 requests (such as clustering operations).
* A client certificate to _authenticate_ requests from `etcd-operator`.
* A client certificate to _authenticate_ requests from Vault.

One will also need a _Certificate Authority_ (CA) to sign these certificates.
Since one will be securing communications in cluster-internal domains (such as
`etcd-0000.etcd-vault.etcd.svc.cluster.local`) one cannot rely on an external
CA to provide these certificates. That being, one must bootstrap their own CA
and use it to sign these certificates.

[NOTE]
====
Since `etcd-operator` has some strict requirements on the format of the
input for TLS configuration, and due to the amount of certificates one needs
to generate, a helper script is provided at `tls/create-etcd-certs.sh`.
Running it will bootstrap the CA and sign all the necessary certificates.
====

To generate the certificates run:

[source,bash]
----
$ ./tls/create-etcd-certs.sh
2017/09/12 18:33:09 [INFO] generating a new CA key and certificate from CSR
(...)
----

[[bookmark-tls-certs]]

[IMPORTANT]
====
This will generate some `.crt` and `.key` files that will be placed in the
`tls/certs` folder. One must make sure that they are kept in a safe place and
safe from prying eyes. Leaking these files may result in a compromised etcd
cluster.
====

[NOTE]
====
The Certificate Authority generated in this step *_is not_* the same thing as
the Certificate Authority one is seeking to establish as a result of this
project. Its only purpose is to establish trust in this particular setup of
etcd and Vault, and it *_must not_* be used for anything else.
====

=== Creating Kubernetes secrets for the TLS certificates

As mentioned above, `etcd-operator` has strict requirements regarding the names
of the certificate files used to establish TLS communications. In particular,
`etcd-operator` expects three Kubernetes secrets to be provided when creating a
new etcd cluster:

[cols="2*"]
|===
^| Secret name
^| Description

^| `etcd-peer-tls`
<| a secret containing a certificate bundle for
 _server-to-server_ communication.
^| `etcd-server-tls`
<| a secret containing a certificate bundle for
 _client-to-server_ communication.
^| `etcd-client-tls`
<| a secret containing a certificate bundle for
 authenticating `etcd-operator` requests.
|===

[NOTE]
====
The structure of each secret is discussed in detail in the `etcd-operator`
https://github.com/coreos/etcd-operator/blob/v0.5.2/doc/user/cluster_tls.md[docs].
In order to ease the creation of these secrets when following this document, a
helper script is provided at `tls/create-etcd-secrets.sh`. Running it will
create all the necessary secrets in the Kubernetes cluster.
====

To create these secrets in the Kubernetes cluster one must run

[source,bash]
----
$ ./tls/create-etcd-secrets.sh
secret "etcd-peer-tls" created
secret "etcd-server-tls" created
secret "etcd-client-tls" created
secret "vault-etcd-tls" created
----

This will also create a `vault-etcd-tls` secret that will be needed later on.

[NOTE]
====
`vault-etcd-tls` secret follows the `etcd-operator` conventions for consistency
and ease of management. In practice it didn't need to follow the same
conventions.
====

[NOTE]
====
At this point one should give <<bookmark-tls-certs,this note>> a second read
and decide what to do with the certificate files in `tls/certs`, as they won't
be needed for the remainder of the procedure.
====

=== Deploying the `etcd-vault` cluster

Now that `etcd-operator` and the necessary Kubernetes secrets are adequately
setup it is time to create the etcd cluster. To do that, one must run

[source,bash]
----
$ kubectl create -f etcd/vault-etcd-etcdcluster.yaml
etcdcluster "etcd-vault" created
----

[NOTE]
====
The cluster is created as a
https://kubernetes.io/docs/concepts/api-extension/custom-resources/[custom resource]
of type `etcdcluster` rather than as a traditional `Deployment` or `ReplicaSet`
— it is `etcd-operator` that will take care of "translating" this
custom resource into a set of pods and services according to the provided
specification.
====

[NOTE]
====
A few words on the link:./etcd/vault-etcd-etcdcluster.yaml[cluster specification]:

* We are requesting a cluster with *_three nodes_*. This is believed to be
 enough to the project's needs.
* We are requesting the cluster to be based on *_etcd v3.1.10_*. This is
 because `etcd-operator` doesn't fully support v3.2 yet.
* We are requesting `etcd-operator` to make backups of our data *_every
 hour_*, and to keep *_at most twelve backups_*. These are stored in a 2GB
 https://cloud.google.com/compute/docs/disks/[Persistent Disk].
* We are requesting for the cluster to be named `etcd-vault`.
====

[TIP]
====
It is now a good idea to check whether the deployment suceeded by inspecting
pods in the `etcd` namespace:

[source,bash]
----
$ kubectl get pod --namespace etcd
NAME                                         READY     STATUS    RESTARTS   AGE
etcd-operator-3662686044-1v9wm               1/1       Running   0          19h
etcd-vault-0000                              1/1       Running   0          6h
etcd-vault-0001                              1/1       Running   0          6h
etcd-vault-0002                              1/1       Running   0          6h
etcd-vault-backup-sidecar-1593014069-237dq   1/1       Running   0          6h
----

If one's output is similar to this it is safe to proceed.
====

== Deploying Vault

Vault's deployment has to be split in three parts:

* One first creates the Vault `Deployment` itself, which creates a Vault
  instance that is _unitialized_ and _sealed_. This means it will not accept
  requests except for the ones required for the initial configuration process.
* One then proceeds to _initializing_ and _unsealing_ Vault. This will leave
  Vault in a state in which it can accept requests.
* One finally exposes the Vault deployment to outside the Kubernetes cluster
  and secures the deployment.

=== Initial deployment

Vault's deployment is composed of six files:

[cols="2*"]
|===

^|File
^|Description

^|`nginx-configmap.yaml`
<|contains Nginx's configuration file
^|`vault-configmap.yaml`
<|contains Vault's
  https://www.vaultproject.io/docs/configuration/index.html[configuration]
  file
^|`vault-serviceaccount.yaml`
<|creates a service account for Vault
^|`vault-deployment.yaml`
<|describes the deployment of Vault itself.
^|`vault-service.yaml`
<|exposes Vault as a service inside the Kubernetes cluster
^|`vault-ingress.yaml`
<|exposes Vault to outside the Kubernetes cluster

|===

[NOTE]
====
Creating a dedicated service account for Vault doesn't bring any immediate
benefit. However, it allows us to follow the principle of least-privilege from
an early stage and to prevent some known issues with `default` service
accounts.
====

In this first part one will be creating the first three resources, leaving the
service and the ingress resources for later. In order to start the deployment
one needs to run the following commands:

[source,bash]
----
$ kubectl create -f vault/nginx-configmap.yaml
configmap "vault" created
----

[source,bash]
----
$ kubectl create -f vault/vault-configmap.yaml
configmap "vault" created
----

[source,bash]
----
$ kubectl create -f vault/vault-serviceaccount.yaml
serviceaccount "vault" created
----

[source,bash]
----
$ kubectl create -f vault/vault-deployment.yaml
deployment "vault" created
----

As mentioned above, this creates a Vault instance that is _unitialized_ and
_sealed_. This means it will not accept requests except for the ones required
for the initial configuration process.

[TIP]
====
It is now a good idea to check whether the deployment suceeded by inspecting
pods in the `vault` namespace. One should wait for a few seconds and run

[source,bash]
----
$ kubectl get pod --namespace vault
NAME                     READY     STATUS    RESTARTS   AGE
vault-2305212354-86j82   1/2       Running   0          1m
----

If one’s output is similar to this it is safe to proceed, even though the
number of ready pods is one. This is because Vault hasn't been initialized and
unsealed yet.
====

[TIP]
====

If one inspects the logs of the Vault container, one will find the following
output:

[source,bash]
----
$ VAULT_POD=$(kubectl get pod --namespace vault \
  | grep vault \
  | head -n 1 \
  | awk '{print $1}')
$ kubectl logs --namespace vault --container vault "$VAULT_POD"
==> Vault server configuration:

                     Cgo: disabled
              Listener 1: tcp (addr: "0.0.0.0:8200", cluster address: "0.0.0.0:8201", tls: "disabled")
               Log Level: info
                   Mlock: supported: true, enabled: true
                 Storage: etcd (HA disabled)
                 Version: Vault v0.8.2
             Version Sha: 9afe7330e06e486ee326621624f2077d88bc9511

==> Vault server started! Log data will stream in below:

2017/09/15 09:04:12.495266 [INFO ] core: security barrier not initialized
2017/09/15 09:04:12.499167 [INFO ] core: security barrier not initialized
2017/09/15 09:04:17.494711 [INFO ] core: security barrier not initialized
2017/09/15 09:04:17.498181 [INFO ] core: security barrier not initialized
----

These `INFO` level messages indicate that Vault hasn't been initialized
yet. Vault will keep repeating these until one takes action.

====

=== Initializing and unsealing Vault

[IMPORTANT]
====
This procedure must be executed by a trusted individual. One will be handling
information that, if leaked, can compromise the security of the data stored by
Vault.
====

Vault must now be initialized and unsealed. As the Vault pod are not accessible
from outside the cluster at this time, one needs to establish port-forwarding
to the local machine. To do that, one should run the following in one terminal
window:

[source,bash]
----
$ VAULT_POD_NAME=$(kubectl get --namespace vault pod \
  | grep vault \
  | awk '{print $1}') // <1>
$ kubectl port-forward --namespace vault "${VAULT_POD_NAME}" 8200:8200 // <2>
Forwarding from 127.0.0.1:8200 -> 8200
Forwarding from [::1]:8200 -> 8200
----
<1> Gets the name of the Vault pod and stores it in the `VAULT_POD_NAME`
    environment variable.
<2> Forwards port `8200` of the Vault pod to the local `8200` pod.

Now, one should leave this command running and open a second terminal window.
In this new window one should run the following commands:

[source,bash]
----
$ export VAULT_ADDR="http://127.0.0.1:8200" // <1>
$ vault init // <2>
Unseal Key 1: +G8hVWrVaOnEQquasRyWdE2RAFuCQumodY6YgzfJzGOD // <3>
Unseal Key 2: XpfepkWVkMWLMJRyranNQDSofE1TjXTJho+ImaozyQ6X // <3>
Unseal Key 3: wfFvslot+7s0ainbE40iIhfSk7L6rs+4prc0pjQzvxtJ // <3>
Unseal Key 4: BhWFOwkg2QTW5DkBfzZWTygWAQ3IA6pMGtUF1i+wUxOr // <3>
Unseal Key 5: iLGQSSJhBqe65zpkliOATGcCe+7d2L0wn5Nl3KO3PZW9 // <3>
Initial Root Token: c689c370-22ec-8268-0ea8-4cbb50c2e00c // <4>

Vault initialized with 5 keys and a key threshold of 3. Please
securely distribute the above keys. When the vault is re-sealed,
restarted, or stopped, you must provide at least 3 of these keys
to unseal it again.

Vault does not store the master key. Without at least 3 keys,
your vault will remain permanently sealed.
----
<1> Sets the value of the `VAULT_ADDR` environment variable to the address
    where the Vault pod is exposed locally.
<2> Initializes Vault.
<3> This instance's _unseal keys_.
<4> This instance's _initial root token_.

At this point it is of extreme importance to:

* safely store the unseal keys shown in the `vault init` command output.
* distribute the unseal keys among trusted individuals, making sure each gets
  only one unseal key.

[TIP]
====
Although it may seem old-fashinoned and paranoid, it is a good idea to use
paper and pen to store the unseal keys, and have each trusted individual store
them in a safe place like a _safe box_.
====

[TIP]
====
One is now considered a trusted individual and, as such, one can keep one
unseal key for themself and distribute the remaining four.
====

Now that Vault is initialized it is time to unseal it so that it can be used.
In the same terminal window where one ran `vault init` one should run

[source,bash]
----
$ vault unseal
Key (will be hidden): // <1>
Sealed: true // <2>
Key Shares: 5
Key Threshold: 3 // <2>
Unseal Progress: 1 // <2>
Unseal Nonce: 0dfa6cf3-abb9-c5cd-4725-21f89cc2feea
----
<1> At this point one should input one of the unseal keys.
<2> The Vault instance is _sealed_, and the _key threshold_ is 3. This means
    that Vault requires the usage of three different unseal keys to complete
    the unseal process.

One should now repeat this command, using two out of the other four unseal
keys. In the last step, one will receive the following output:

[source,bash]
----
$ vault unseal
Key (will be hidden): // <1>
Sealed: false // <2>
Key Shares: 5
Key Threshold: 3
Unseal Progress: 0
Unseal Nonce:
----
<1> At this point one should input one of the unseal keys not yet used.
<2> The Vault instance is _unsealed_.

Vault is now unsealed and ready to serve requests. However, there is one last
step one should do before proceeding — to _revoke_ the initial root token.
While this may seem counter-intuitive it is in fact a recommended practice. In
the same terminal window run:

[source,bash]
----
$ vault auth c689c370-22ec-8268-0ea8-4cbb50c2e00c // <1>
Successfully authenticated! You are now logged in.
token: c689c370-22ec-8268-0ea8-4cbb50c2e00c // <1>
token_duration: 0
token_policies: [root]
----

[source,bash]
----
$ vault token-revoke c689c370-22ec-8268-0ea8-4cbb50c2e00c // <1>
Success! Token revoked if it existed.
----
<1> This corresponds to the _initial root token_.

Vault is now initialized and unsealed, and the initial root token has been
revoked. It is now time to continue the deployment by exposing the Vault
deployment to outside the Kubernetes cluster.

[TIP]
====
One may now return to the terminal window where `kubectl port-forward` is
running and terminate the process using Ctrl-C.
====

[TIP]
====
If one inspects pods in the `vault` namespace one will find that they are now
marked as _Ready_. This is because the Vault instance is now initialized and
unsealed.

[source,bash]
----
$ kubectl get pod --namespace vault
NAME                     READY     STATUS    RESTARTS   AGE
vault-4120637469-hcwtn   2/2       Running   0          5m
----
====

=== Exposing Vault

One will now expose Vault to outside the cluster, so that applications running
in other clusters can access it. To do this one needs to create a global static
IP in GCP:

[source,bash]
----
$ gcloud compute addresses create vault --global
Created [https://www.googleapis.com/compute/v1/projects/<project-name>/global/addresses/vault].
----

[source,bash]
----
$ gcloud compute addresses describe vault --global
address: 35.201.114.242 // <1>
creationTimestamp: '2017-09-18T01:52:08.285-07:00'
description: ''
id: '3141996435895105495'
ipVersion: IPV4
kind: compute#address
name: vault // <2>
selfLink: https://www.googleapis.com/compute/v1/projects/<project-name>/global/addresses/vault
status: IN_USE
users:
- https://www.googleapis.com/compute/v1/projects/<project-name>/global/forwardingRules/k8s-fw-vault-vault--cdf21de8575bf56e
----
<1> The IP address one will use to expose Vault.
<2> The name of the IP address one will use to expose Vault.

NOTE: If one creates the IP address with a different name one must update the
`vault/vault-ingress.yaml` file accordingly.

After the `vault` IP address is created, one must configure the DNS of the
domain one is going to use to expose Vault. For instance, if one wants to
expose Vault at `https://vault.example.com` one has to create a DNS record with
type `A` and name `vault` pointing to the abovementioned IP address at the DNS
provider for the `example.com` domain. The steps to do this are highly
dependent on the DNS provider for the domain and cannot be detailed here.

[IMPORTANT]
====
From this point it is assumed that DNS has been properly configured
and that changes have propagated. One can test whether changes have propagated
by using `dig`:

[source,bash]
----
dig @8.8.8.8 vault.example.com A // <1>

; <<>> DiG 9.8.3-P1 <<>> @8.8.8.8 vault.example.com A
; (1 server found)
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 43874
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0

;; QUESTION SECTION:
;vault.example.space.		IN	A

;; ANSWER SECTION:
vault.example.space.	299	IN	A	35.201.114.242 // <2>

;; Query time: 61 msec
;; SERVER: 8.8.8.8#53(8.8.8.8)
;; WHEN: Mon Sep 18 12:10:39 2017
;; MSG SIZE  rcvd: 53
----
<1> Looks-up `A` records for `vault.example.com` at Google Public DNS (`8.8.8.8`).
<2> This must match the global IP address created above.

It is highly recommended to wait for changes to propagate before proceeding.
====

[IMPORTANT]
====
Before running the following commands one should update the
`vault/vault-ingress.yaml` file with the actual domain name used to expose
Vault.
====

Once the `vault` IP address is created one must create the service and ingress
resources:

[source,bash]
----
$ kubectl create -f vault/vault-service.yaml
service "vault" created
----

[source,bash]
----
$ kubectl create -f vault/vault-ingress.yaml
ingress "vault" created
----

[NOTE]
====
This will create a
https://cloud.google.com/compute/docs/load-balancing/[_global *external* load-balancer_]
pointing to the Vault deployment.
====

In order to secure Vault external access one must now configure HTTPS access.
The easiest and cheapest way to obtain a trusted TLS certicate is using
https://letsencrypt.org/[Let's Encrypt], and the easiest way to automate the
process of obtaining and renewing certificates from Let's Encrypt is by using
https://github.com/jetstack/kube-lego[`kube-lego`]:

[IMPORTANT]
====
Before running the following commands one should update the
`kube-lego/kube-lego.yaml` file with one's actual email address.
====

[source,bash]
----
$ kubectl create -f ./kube-lego/kube-lego-bundle.yaml
namespace "kube-lego" created
configmap "kube-lego" created
deployment "kube-lego" created
----

[TIP]
====
As soon as it starts, `kube-lego` will start monitoring ingress resources and
requesting certificates from Let's Encrypt. One can check that the deployment
succeeded by following these steps:

[source,bash]
----
$ KUBE_LEGO_POD_NAME=$(kubectl get --namespace kube-lego pod \
  | grep kube-lego \
  | awk '{print $1}')
----

[source,bash]
----
$ kubectl logs --namespace kube-lego "${KUBE_LEGO_POD_NAME}"
time="2017-09-18T11:22:05Z" level=info msg="kube-lego 0.1.5-a9592932 starting" context=kubelego
time="2017-09-18T11:22:05Z" level=info msg="connecting to kubernetes api: https://10.15.240.1:443" context=kubelego
time="2017-09-18T11:22:05Z" level=info msg="successfully connected to kubernetes api v1.7.5" context=kubelego
time="2017-09-18T11:22:05Z" level=info msg="server listening on http://:8080/" context=acme
(...)
----

====

[NOTE]
====
Let's Encrypt must be able to reach port `80` on domains for which certificates
are requested, hence the annotation `kubernetes.io/ingress.allow-http`
in `vault/vault-ingress.yaml` must be set to `"true"`.
====

[NOTE]
====
It is safe to set the abovementioned annotation to `"true"`—the Nginx instance
that is deployed alongside Vault makes sure that Vault only communicates over
HTTPS. Any request to Vault via HTTP will be rejected.
====

If everything goes well, after a while one will be able to access
https://vault.example.com/v1/sys/health securely. On the other hand, if one
tries to access http://vault.example.com/v1/sys/health one will get an error
response.
