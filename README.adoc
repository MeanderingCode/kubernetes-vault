= [Draft] kubernetes-vault
Vault 0.8.3 on top of Kubernetes (backed by etcd 3.1.10).
:icons: font
:toc:

ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

== Pre-Requisites

* A GCP project in which one has the following Google Cloud IAM roles:
** _Container_ > _Container Engine Admin_
** _Compute Engine_ > _Compute Engine Admin_
* A working GKE cluster running Kubernetes v1.7.5 with legacy authorization
  *_disabled_*.
* A working installation of `gcloud` configured to access the target Google
  Cloud Platform project.
* A working installation of `kubectl` configured to access the target cluster.
* A working installation of https://github.com/cloudflare/cfssl[`cfssl`]
  (more details <<#bookmark-cfssl, below>>).
* The https://https://www.vaultproject.io/[Vault] binary (more
  details <<#bookmark-vault-binary, below>>).

== Before proceeding

[IMPORTANT]
====
Disabling legacy authorization means that _Role-Based Access Control_
(https://kubernetes.io/docs/admin/authorization/rbac/[RBAC]) is enabled. RBAC
is the new way to manage permissions in Kubernetes 1.6+. Reading the RBAC
documentation and understanding the differences between RBAC and legacy
authorization is strongly encouraged.
====

[IMPORTANT]
====
One must grant themselves the `cluster-admin` role *manually* before
proceeding. This is due to a known issue with RBAC on GKE. To do that one must
retrieve their identity using `gcloud` by running

[source,bash]
----
$ MY_GCLOUD_USER=$(gcloud info \
  | grep Account \
  | awk -F'[][]' '{print $2}')
----

and then create a `ClusterRoleBinding` by running

[source,bash]
----
$ kubectl create clusterrolebinding \
  <cluster-role-binding-name> \
  --clusterrole=cluster-admin \
  --user=${MY_GCLOUD_USER}
----

One must replace `<cluster-role-binding-name>` in the above command with a
unique, meaningful name. It is a good idea to include one's identity in the
name (e.g., `j.doe-cluster-admin`).
====

[[bookmark-cfssl]]
[IMPORTANT]
====
`cfssl` can be installed either

* from https://github.com/cloudflare/cfssl[source] — this requires a Go 1.6+
  environment setup on one's workstation; or
* by downloading the https://pkg.cfssl.org/[binaries] for one's architecture
  and dropping them on `${PATH}`.

If downloading the binaries one needs only to download `cfssl` and `cfssljson`.
====

[IMPORTANT]
====
Vault can be installed by downloading the
https://www.vaultproject.io/downloads.html[binary] for one's architecture and
dropping it on `${PATH}`. The binary can act both as server and client, and
will be needed later on to configure the Vault deployment.
====

[IMPORTANT]
====
All commands should be run in a terminal and from the root of the repository.
====

== Creating the `etcd` and `vault` namespaces

To better group and manage the major components of the solution it is
recommended to create two namespaces in the cluster — one for etcd-related
resources and another one for Vault-related ones:

[source,bash]
----
$ kubectl create namespace etcd
namespace "etcd" created
----

[source,bash]
----
$ kubectl create namespace vault
namespace "vault" created
----

[IMPORTANT]
====
If one does not follow this approach, or if one chooses different names for the
namespaces, one must update the scripts and Kubernetes descriptors in this
repository accordingly.
====

== Deploying etcd

=== Deploying `etcd-operator`

`etcd-operator` will be responsible for managing the etcd cluster that Vault
will use as storage backend. It will handle tasks such as
<<#bookmark-etcd-vault-periodic-backups,periodic backups>> and member recovery
in disaster scenarios. `etcd-operator` and the cluster itself will live in the
`etcd` namespace.

To start with, and since RBAC is active on the cluster, one needs to setup
adequate permissions. To do this one needs to

* create a `ClusterRole` specifying a list of permissions;
* create a dedicated `ServiceAccount` for `etcd-operator`;
* create a `CluserRoleBinding` that grants these permissions to the service
 account.

This is made by running the following commands:

[source,bash]
----
$ kubectl create -f ./etcd-operator/etcd-operator-clusterrole.yaml
clusterrole "etcd-operator" created
----

[source,bash]
----
$ kubectl create -f ./etcd-operator/etcd-operator-serviceaccount.yaml
serviceaccount "etcd-operator" created
----

[source,bash]
----
$ kubectl create -f ./etcd-operator/etcd-operator-clusterrolebinding.yaml
clusterrolebinding "etcd-operator" created
----

One is now ready to deploy `etcd-operator` itself:

[source,bash]
----
$ kubectl create -f ./etcd-operator/etcd-operator-deployment.yaml
deployment "etcd-operator" created
----

[TIP]
====
At this point it is a good idea to check whether the deployment succeeded. One
should wait for a few seconds and then run:

[source,bash]
----
$ ETCD_OPERATOR_POD_NAME=$(kubectl get pod --namespace etcd \
  | grep etcd-operator \
  | awk 'NR==1' \
  | awk '{print $1}')
----

[source,bash]
----
$ kubectl logs --follow --namespace etcd "${ETCD_OPERATOR_POD_NAME}"
time="2017-09-26T10:48:43Z" level=info msg="etcd-operator Version: 0.6.0"
time="2017-09-26T10:48:43Z" level=info msg="Git SHA: a04c308"
time="2017-09-26T10:48:43Z" level=info msg="Go Version: go1.9"
time="2017-09-26T10:48:43Z" level=info msg="Go OS/Arch: linux/amd64"
time="2017-09-26T10:48:43Z" level=info msg="Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"etcd", Name:"etcd-operator", UID:"43a0bc38-a2a8-11e7-9f73-42010a9a0fc9", APIVersion:"v1", ResourceVersion:"2111", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' etcd-operator-3796541610-99ptj became leader"
----

If the output doesn't differ much from the example, and in particular if one
doesn't see any error messages, it is safe to proceed.
====

=== Generating TLS certificates for etcd

One is now almost ready to create the etcd cluster that will back the Vault
deployment. However, before proceeding, one needs to generate TLS certificates
to secure communications with the etcd cluster.

[NOTE]
====
Even though the etcd cluster won't be exposed to outside the Kubernetes
cluster, and even though Vault encrypts all data before it reaches the
network, it is always a good security measure to enable TLS communication in
the cluster.
====

One will need different types of certificates for establishing TLS:

* A server certificate which etcd will use for serving _client-to-server_
 requests (such as a request for a key).
* A server certificate which etcd will use for serving _server-to-server_
 requests (such as clustering operations).
* A client certificate to _authenticate_ requests from `etcd-operator`.
* A client certificate to _authenticate_ requests from Vault.

One will also need a _Certificate Authority_ (CA) to sign these certificates.
Since one will be securing communications in cluster-internal domains (such as
`etcd-0000.etcd-vault.etcd.svc.cluster.local`) one cannot rely on an external
CA to provide these certificates. That being, one must bootstrap their own CA
and use it to sign these certificates.

[NOTE]
====
Since `etcd-operator` has some strict requirements on the format of the
input for TLS configuration, and due to the amount of certificates one needs
to generate, a helper script is provided at `tls/create-etcd-certs.sh`.
Running it will bootstrap the CA and sign all the necessary certificates.
====

To generate the certificates run:

[source,bash]
----
$ ./tls/create-etcd-certs.sh
2017/09/18 13:05:34 [INFO] generating a new CA key and certificate from CSR
(...)
----

[[bookmark-tls-certs]]

[IMPORTANT]
====
This will generate some `.crt` and `.key` files that will be placed in the
`tls/certs` folder. One must make sure that they are kept in a safe place and
safe from prying eyes. Leaking these files may result in a compromised etcd
cluster.
====

[NOTE]
====
The Certificate Authority generated in this step *_is not_* the same thing as
the Certificate Authority one is seeking to establish as a result of this
project. Its only purpose is to establish trust in this particular setup of
etcd and Vault, and it *_must not_* be used for anything else.
====

=== Creating Kubernetes secrets for the TLS certificates

As mentioned above, `etcd-operator` has strict requirements regarding the names
of the certificate files used to establish TLS communications. In particular,
`etcd-operator` expects three Kubernetes secrets to be provided when creating a
new etcd cluster:

[cols="2*"]
|===
^| Secret name
^| Description

^| `etcd-peer-tls`
<| a secret containing a certificate bundle for
 _server-to-server_ communication.
^| `etcd-server-tls`
<| a secret containing a certificate bundle for
 _client-to-server_ communication.
^| `etcd-client-tls`
<| a secret containing a certificate bundle for
 authenticating `etcd-operator` requests.
|===

[NOTE]
====
The structure of each secret is discussed in detail in the `etcd-operator`
https://github.com/coreos/etcd-operator/blob/v0.6.0/doc/user/cluster_tls.md[docs].
In order to ease the creation of these secrets when following this document, a
helper script is provided at `tls/create-etcd-secrets.sh`. Running it will
create all the necessary secrets in the Kubernetes cluster.
====

To create these secrets in the Kubernetes cluster one must run

[source,bash]
----
$ ./tls/create-etcd-secrets.sh
secret "etcd-peer-tls" created
secret "etcd-server-tls" created
secret "etcd-client-tls" created
secret "vault-etcd-tls" created
----

This will also create a `vault-etcd-tls` secret that will be needed later on.

[NOTE]
====
`vault-etcd-tls` secret follows the `etcd-operator` conventions for consistency
and ease of management. In practice it didn't need to follow the same
conventions.
====

[NOTE]
====
At this point one should give <<bookmark-tls-certs,this note>> a second read
and decide what to do with the certificate files in `tls/certs`, as they won't
be needed for the remainder of the procedure.
====

=== Deploying the `etcd-vault` cluster

Now that `etcd-operator` and the necessary Kubernetes secrets are adequately
setup it is time to create the etcd cluster. To do that, one must run

[source,bash]
----
$ kubectl create -f etcd/vault-etcd-etcdcluster.yaml
etcdcluster "etcd-vault" created
----

[NOTE]
====
The cluster is created as a
https://kubernetes.io/docs/concepts/api-extension/custom-resources/[custom resource]
of type `etcdcluster` rather than as a traditional `Deployment` or `ReplicaSet`
— it is `etcd-operator` that will take care of "translating" this
custom resource into a set of pods and services according to the provided
specification.
====

[[bookmark-etcd-vault-periodic-backups]]

[NOTE]
====
A few words on the link:./etcd/vault-etcd-etcdcluster.yaml[cluster specification]:

* We are requesting a cluster with *_three nodes_*. This is believed to be
 enough to the project's needs.
* We are requesting the cluster to be based on *_etcd v3.1.10_*. This is
 because `etcd-operator` doesn't fully support v3.2 yet.
* We are requesting `etcd-operator` to make backups of our data *_every
 hour_*, and to keep *_at most twelve backups_*. These are stored in a 2GB
 https://cloud.google.com/compute/docs/disks/[Persistent Disk].
* We are requesting for the cluster to be named `etcd-vault`.
====

[TIP]
====
It is now a good idea to check whether the deployment suceeded by inspecting
pods in the `etcd` namespace:

[source,bash]
----
$ kubectl get pod --namespace etcd
NAME                                        READY     STATUS    RESTARTS   AGE
etcd-operator-3160827283-2v367              1/1       Running   0          2m
etcd-vault-0000                             1/1       Running   0          47s
etcd-vault-0001                             1/1       Running   0          39s
etcd-vault-0002                             1/1       Running   0          21s
etcd-vault-backup-sidecar-899031248-85xck   1/1       Running   0          47s
----

If one's output is similar to this it is safe to proceed.
====

== Deploying Vault

Vault's deployment has to be split in three parts:

* One first creates the Vault `StatefulSet` itself, which creates two Vault
  instances that are _uninitialized_ and _sealed_. This means they will not
  accept any requests except for the ones required for the initial
  configuration process.
* One then proceeds to _initializing_ the Vault storage backend and _unsealing_
  the two Vault instances. This will leave Vault in a state in which it can
  accept requests.
* One finally exposes the Vault deployment to outside the Kubernetes cluster
  and secures the deployment.

=== Initial deployment

Vault's deployment is composed of seven files:

[cols="2*"]
|===

^|File
^|Description

^|`nginx-configmap.yaml`
<|contains Nginx's configuration file
^|`vault-configmap.yaml`
<|contains Vault's
  https://www.vaultproject.io/docs/configuration/index.html[configuration]
  file
^|`vault-serviceaccount.yaml`
<|creates a service account for Vault
^|`vault-service.yaml`
<|exposes Vault as a service inside the Kubernetes cluster (both for API
  requests and clustering)
^|`vault-statefulset.yaml`
<|describes the deployment of Vault itself
^|`vault-api-service.yaml`
<|creates a `NodePort` service that exposes the Vault API
^|`vault-api-ingress.yaml`
<|exposes the Vault API to outside the Kubernetes cluster

|===

[NOTE]
====
Creating a dedicated service account for Vault doesn't bring any immediate
benefit. However, it allows us to follow the principle of least-privilege from
an early stage and to prevent some known issues with `default` service
accounts.
====

[NOTE]
====
The *_headless service_* service defined in `vault-service.yaml` supports
both the `StatefulSet` defined in `vault-statefulset.yaml` as well as
clustering and high-availability of the Vault deployment.
====

[NOTE]
====
One must create the `vault-api-service.yaml` service to support the ingress
resource in GCP, since the GCE ingress controller requires a service of type
`NodePort` to be created.
====

In this first part one will be creating the first five resources, leaving the
second service and the ingress resources for later. In order to start the
deployment one needs to run the following commands:

[IMPORTANT]
====
Before running the following commands one should update the
`vault/vault-configmap.yaml` file with the address where Vault will be made
publicly accessible (check <<#bookmark-vault-address,below>>).
====

[source,bash]
----
$ kubectl create -f vault/nginx-configmap.yaml
configmap "vault" created
----

[source,bash]
----
$ kubectl create -f vault/vault-configmap.yaml
configmap "vault" created
----

[source,bash]
----
$ kubectl create -f vault/vault-serviceaccount.yaml
serviceaccount "vault" created
----

[source,bash]
----
$ kubectl create -f vault/vault-service.yaml
service "vault" created
----

[source,bash]
----
$ kubectl create -f vault/vault-statefulset.yaml
statefulset "vault" created
----

As mentioned above, this will create two Vault instances that are
_uninitialized_ and _sealed_. This means that they will not accept requests
except for the ones required for the initial configuration process.

[TIP]
====
It is now a good idea to check whether the deployment suceeded by inspecting
pods in the `vault` namespace. One should wait for a few seconds and run

[source,bash]
----
$ kubectl get pod --namespace vault
NAME      READY     STATUS    RESTARTS   AGE
vault-0   1/2       Running   0          30s
vault-1   1/2       Running   0          30s
----

If one’s output is similar to this it is safe to proceed to the next section,
*even though the pods are not in the `Ready` state*. This happens because
Vault hasn't been initialized and unsealed yet.
====

[TIP]
====
Going a bit deeper, the behavior described above happens because a
`ReadinessProbe` is set on each Vault pod. This probe will only mark a pod as
`Ready` when these two conditions are satisfied:

* The Vault storage has been *_initialized_*.
* That particular Vault instance has been *_unsealed_*.
====

[TIP]
====

If one inspects the logs of a Vault container, say `vault-0`, one will find the
following output:

[source,bash]
----
$ kubectl logs --namespace vault --container vault vault-0
==> Vault server configuration:

                     Cgo: disabled
         Cluster Address: https://vault:8201
              Listener 1: tcp (addr: "0.0.0.0:8200", cluster address: "0.0.0.0:8201", tls: "disabled")
               Log Level: info
                   Mlock: supported: true, enabled: true
        Redirect Address: https://vault.example.com
                 Storage: etcd (HA available)
                 Version: Vault v0.8.3
             Version Sha: 6b29fb2b7f70ed538ee2b3c057335d706b6d4e36

==> Vault server started! Log data will stream in below:

(...)
2017/09/20 15:01:55.772792 [INFO ] core: security barrier not initialized
2017/09/20 15:02:00.774962 [INFO ] core: security barrier not initialized
2017/09/20 15:02:05.770796 [INFO ] core: security barrier not initialized
2017/09/20 15:02:10.768767 [INFO ] core: security barrier not initialized
----

These `INFO` level messages indicate that Vault hasn't been initialized
yet. Vault will keep repeating these until one takes action.
====

=== Initializing and unsealing Vault

[IMPORTANT]
====
This procedure must be executed by a trusted individual. One will be handling
information that, if leaked, can compromise the security of the data stored by
Vault.
====

Vault must now be initialized, and both instances must be unsealed. As the
Vault pods are not accessible from outside the cluster at this time, one needs
to establish port-forwarding to one's local workstation. To do that, one should
run the following in one terminal window:

[source,bash]
----
$ kubectl port-forward --namespace vault vault-0 18200:8200 // <1>
Forwarding from 127.0.0.1:18200 -> 8200
Forwarding from [::1]:18200 -> 8200
----
<1> Forwards port `8200` of the first Vault pod to the local `18200` port.

Now, one should leave this command running and open a second terminal window.
In this new window one should run the following commands:

[source,bash]
----
$ export VAULT_ADDR="http://127.0.0.1:18200" // <1>
$ vault init // <2>
Unseal Key 1: +G8hVWrVaOnEQquasRyWdE2RAFuCQumodY6YgzfJzGOD // <3>
Unseal Key 2: XpfepkWVkMWLMJRyranNQDSofE1TjXTJho+ImaozyQ6X // <3>
Unseal Key 3: wfFvslot+7s0ainbE40iIhfSk7L6rs+4prc0pjQzvxtJ // <3>
Unseal Key 4: BhWFOwkg2QTW5DkBfzZWTygWAQ3IA6pMGtUF1i+wUxOr // <3>
Unseal Key 5: iLGQSSJhBqe65zpkliOATGcCe+7d2L0wn5Nl3KO3PZW9 // <3>
Initial Root Token: c689c370-22ec-8268-0ea8-4cbb50c2e00c // <4>

Vault initialized with 5 keys and a key threshold of 3. Please
securely distribute the above keys. When the vault is re-sealed,
restarted, or stopped, you must provide at least 3 of these keys
to unseal it again.

Vault does not store the master key. Without at least 3 keys,
your vault will remain permanently sealed.
----
<1> Sets the value of the `VAULT_ADDR` environment variable to the address
    where the first Vault pod is exposed locally.
<2> Initializes Vault.
<3> Vault's _unseal keys_.
<4> Vault's _initial root token_.

At this point it is of extreme importance to:

* safely store the unseal keys shown in the `vault init` command output.
* distribute the unseal keys among trusted individuals, making sure each gets
  only one unseal key.

[TIP]
====
Although it may seem old-fashinoned and paranoid, it is a good idea to use
paper and pen to store the unseal keys, and have each trusted individual store
them in a safe place like a _safe box_.
====

[TIP]
====
One is now considered a trusted individual and, as such, one can keep one
unseal key for themself and distribute the remaining four.
====

Now that Vault is initialized it is time to unseal it so that it can be used.
In the same terminal window where one ran `vault init` one should run

[source,bash]
----
$ vault unseal
Key (will be hidden): // <1>
Sealed: true // <2>
Key Shares: 5
Key Threshold: 3 // <2>
Unseal Progress: 1 // <2>
Unseal Nonce: 0dfa6cf3-abb9-c5cd-4725-21f89cc2feea
----
<1> At this point one should input one of the unseal keys.
<2> The Vault instance is _sealed_, and the _key threshold_ is 3. This means
    that Vault requires the usage of three different unseal keys to complete
    the unseal process.

One should now repeat this command, using one of the other four unseal keys:

[source,bash]
----
$ vault unseal
Key (will be hidden): // <1>
Sealed: false // <2>
Key Shares: 5
Key Threshold: 3
Unseal Progress: 0
Unseal Nonce:
----
<1> At this point one should input one of the unseal keys not yet used.
<2> The Vault instance is still _unsealed_.

One should now repeat this command again. One will now receive the following
output:

[source,bash]
----
$ vault unseal
Key (will be hidden): // <1>
Sealed: false // <2>
Key Shares: 5
Key Threshold: 3
Unseal Progress: 0
Unseal Nonce:
----
<1> At this point one should input one of the unseal keys not yet used.
<2> The Vault instance is _unsealed_.

The first Vault pod is now unsealed and ready to serve requests.

[TIP]
====
Inspecting pods in the `vault` namespace should now output something similar to

[source,bash]
----
$ kubectl get --namespace vault pod
NAME      READY     STATUS    RESTARTS   AGE
vault-0   2/2       Running   0          7m
vault-1   1/2       Running   0          7m
----
====

Now, one must also _unseal_ the second Vault instance. One should get back to
the *_first terminal window_* — where `kubectl port-forward` is running — and
stop the running process (using `Ctrl-C`). Then, one should run

[source,bash]
----
$ kubectl port-forward --namespace vault vault-1 28200:8200 // <1>
Forwarding from 127.0.0.1:28200 -> 8200
Forwarding from [::1]:28200 -> 8200
----
<1> Forwards port `8200` of the first Vault pod to the local `28200` port.

Now one should get back to the *_second terminal window_* — where
`vault init` and `vault unseal` were run — and run the following commands:

[source,bash]
----
$ export VAULT_ADDR="http://127.0.0.1:28200" // <1>
$ vault unseal
Key (will be hidden): // <2>
(...)
$ vault unseal
Key (will be hidden): // <3>
(...)
$ vault unseal
Key (will be hidden): // <4>
Sealed: false
Key Shares: 5
Key Threshold: 3
Unseal Progress: 0
Unseal Nonce:
----
<1> Sets the value of the `VAULT_ADDR` environment variable to the address
    where the second Vault pod is exposed locally.
<2> At this point one should input one of the unseal keys.
<3> At this point one should input another one of the unseal keys.
<4> At this point one should input yet another one of the unseal keys.

The second Vault pod is now unsealed and ready to serve requests.

[TIP]
====
Inspecting pods in the `vault` namespace should now output something similar to

[source,bash]
----
$ kubectl get --namespace vault pod
NAME      READY     STATUS    RESTARTS   AGE
vault-0   2/2       Running   0          7m
vault-1   1/2       Running   0          7m
----
====

[TIP]
====
If one inspects the logs of the `vault-1` pod one will see something similar to
this:

[source,bash]
----
$ kubectl logs --container vault --namespace vault vault-1
(...)
2017/09/20 15:17:23.263728 [INFO ] core: vault is unsealed
2017/09/20 15:17:23.263728 [INFO ] core: entering standby mode
----

The last log message indicates that this Vault instance will operate as a
*_standby_*. This means it will accept requests and forward them to the
cluster leader (which, in this case, is `vault-0`). To learn more about
clustering and high-availability in Vault one should head over to
https://www.vaultproject.io/docs/concepts/ha.html[Vault's HA documentation].
====

There is one last step one should do before proceeding. We need to _revoke_ the
initial root token. While this may seem counter-intuitive it is in fact a
recommended practice. In the same terminal window where on ran the last 
`vault unseal` command one should run:

[source,bash]
----
$ vault auth c689c370-22ec-8268-0ea8-4cbb50c2e00c // <1>
Successfully authenticated! You are now logged in.
token: c689c370-22ec-8268-0ea8-4cbb50c2e00c // <1>
token_duration: 0
token_policies: [root]
----

[source,bash]
----
$ vault token-revoke c689c370-22ec-8268-0ea8-4cbb50c2e00c // <1>
Success! Token revoked if it existed.
----
<1> This corresponds to the _initial root token_.

The Vault deployment is now initialized, both instances are unsealed, and the
initial root token has been revoked. It is now time to continue the deployment
by exposing the Vault deployment to outside the Kubernetes cluster.

[TIP]
====
One may now return to the terminal window where `kubectl port-forward` is
running and terminate the process using Ctrl-C.
====

[IMPORTANT]
====
The *_unseal_* procedure must be performed every time a Vault pod crashes or
restarts for some reason.
====

=== Exposing Vault

One will now expose Vault to outside the cluster, so that applications running
in other clusters can access it. To do this one needs to create a global static
IP in GCP:

[source,bash]
----
$ gcloud compute addresses create vault --global
Created [https://www.googleapis.com/compute/v1/projects/<project-name>/global/addresses/vault].
----

[source,bash]
----
$ gcloud compute addresses describe vault --global
address: 35.201.114.242 // <1>
creationTimestamp: '2017-09-18T05:12:33.928-07:00'
description: ''
id: '7579662126224115422'
ipVersion: IPV4
kind: compute#address
name: vault // <2>
selfLink: https://www.googleapis.com/compute/v1/projects/<project-name>/global/addresses/vault
status: RESERVED
----
<1> The IP address one will use to expose Vault.
<2> The name of the IP address one will use to expose Vault.

NOTE: If one creates the IP address with a different name one must update the
`vault/vault-api-ingress.yaml` file accordingly.

[[bookmark-vault-address]]

After the `vault` IP address is created, one must configure the DNS of the
domain one is going to use to expose Vault. For instance, if one wants to
expose Vault at `https://vault.example.com` one has to create a DNS record with
type `A` and name `vault` pointing to the abovementioned IP address at the DNS
provider for the `example.com` domain. The steps to do this are highly
dependent on the DNS provider for the domain and cannot be detailed here.

[IMPORTANT]
====
From this point it is assumed that DNS has been properly configured
and that changes have propagated. One can test whether changes have propagated
by using `dig`:

[source,bash]
----
dig @8.8.8.8 vault.example.com A // <1>

; <<>> DiG 9.8.3-P1 <<>> @8.8.8.8 vault.example.com A
; (1 server found)
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 43874
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0

;; QUESTION SECTION:
;vault.example.space.		IN	A

;; ANSWER SECTION:
vault.example.space.	299	IN	A	35.201.114.242 // <2>

;; Query time: 61 msec
;; SERVER: 8.8.8.8#53(8.8.8.8)
;; WHEN: Mon Sep 18 13:13:49 2017
;; MSG SIZE  rcvd: 53
----
<1> Looks-up `A` records for `vault.example.com` at Google Public DNS (`8.8.8.8`).
<2> This must match the global IP address created above.

It is highly recommended to wait for changes to propagate before proceeding.
====

[IMPORTANT]
====
Before running the following commands one should update the
`vault/vault-api-ingress.yaml` file with the actual domain name used to expose
Vault.
====

Once the `vault` IP address is created one must create the service and ingress
resources:

[source,bash]
----
$ kubectl create -f vault/vault-api-service.yaml
service "vault" created
----

[source,bash]
----
$ kubectl create -f vault/vault-api-ingress.yaml
ingress "vault" created
----

[NOTE]
====
This will create a
https://cloud.google.com/compute/docs/load-balancing/[_global *external* load-balancer_]
pointing to the Vault deployment.
====

In order to secure Vault external access one must now configure HTTPS access.
The easiest and cheapest way to obtain a trusted TLS certicate is using
https://letsencrypt.org/[Let's Encrypt], and the easiest way to automate the
process of obtaining and renewing certificates from Let's Encrypt is by using
https://github.com/jetstack/kube-lego[`kube-lego`]:

[IMPORTANT]
====
Before running the following commands one should update the
`kube-lego/kube-lego.yaml` file with one's actual email address.
====

[source,bash]
----
$ kubectl create -f ./kube-lego/kube-lego-bundle.yaml
namespace "kube-lego" created
configmap "kube-lego" created
deployment "kube-lego" created
----

[TIP]
====
As soon as it starts, `kube-lego` will start monitoring ingress resources and
requesting certificates from Let's Encrypt. One can check that the deployment
succeeded by following these steps:

[source,bash]
----
$ KUBE_LEGO_POD_NAME=$(kubectl get --namespace kube-lego pod \
  | grep kube-lego \
  | awk 'NR==1' \
  | awk '{print $1}')
----

[source,bash]
----
$ kubectl logs --namespace kube-lego "${KUBE_LEGO_POD_NAME}"
time="2017-09-18T12:16:06Z" level=info msg="kube-lego 0.1.5-a9592932 starting" context=kubelego
time="2017-09-18T12:16:06Z" level=info msg="connecting to kubernetes api: https://10.15.240.1:443" context=kubelego
time="2017-09-18T12:16:06Z" level=info msg="successfully connected to kubernetes api v1.7.5" context=kubelego
time="2017-09-18T12:16:06Z" level=info msg="server listening on http://:8080/" context=acme
(...)
----

====

[NOTE]
====
Let's Encrypt must be able to reach port `80` on domains for which certificates
are requested, so one must use the

[source,yaml]
----
kubernetes.io/ingress.allow-http: "true"
----

annotation in `vault/vault-api-ingress.yaml`. Please note that it is safe to
set the abovementioned annotation —the Nginx instance that is deployed
alongside Vault makes sure that Vault only communicates over HTTPS.
Any request to Vault via plain HTTP will be rejected.
====

[TIP]
====
If everything goes well, after a short while one will be able to access

https://vault.example.com/v1/sys/health

securely. On the other hand, one will not be able to access
http://vault.example.com/v1/sys/health.
====
