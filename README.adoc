= kubernetes-vault
Run Hashicorp Vault on top of Kubernetes (GKE). Includes instructions for automated backups (GCS) and day-to-day usage.
:icons: font
:toc:

ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

== Pre-Requisites

* A GCP project with the _Google Cloud Key Management Service (KMS) API_
*_enabled_*.
* Having the following Google Cloud IAM roles in this project:
** _Cloud KMS_ > _Cloud KMS Admin_
** _Cloud KMS_ > _Cloud KMS CryptoKey Encrypter/Decrypter_
** _Container_ > _Container Engine Admin_
** _Compute Engine_ > _Compute Network Admin_
* A working GKE cluster running Kubernetes *_v1.8.1_* with legacy authorization
  *_disabled_*.
* A working installation of `gcloud` configured to access the target Google
  Cloud Platform project.
* A working installation of `kubectl` configured to access the target cluster.
* A working installation of https://github.com/cloudflare/cfssl[`cfssl`]
  (more details <<#bookmark-cfssl, below>>).
* A working installation of https://stedolan.github.io/jq/[`jq`]
  (more details <<#bookmark-jq, below>>).
* The https://https://www.vaultproject.io/[Vault] binary (more
  details <<#bookmark-vault-binary, below>>).

== Before proceeding

[IMPORTANT]
====
The steps described below are **mandatory**.
====

[IMPORTANT]
====
All commands should be run from the root of this repository.
====

=== Disable GKE legay authorization mode

Disabling legacy authorization means that _Role-Based Access Control_
(https://kubernetes.io/docs/admin/authorization/rbac/[RBAC]) is enabled. RBAC
is the new way to manage permissions in Kubernetes 1.6+. Reading the RBAC
documentation and understanding the differences between RBAC and legacy
authorization is strongly encouraged.

=== Setup RBAC

Due to a https://cloud.google.com/container-engine/docs/role-based-access-control#defining_permissions_in_a_role[known issue with RBAC on GKE], one must grant themselves the `cluster-admin` role *manually* before
proceeding. In order to do so, one must retrieve their identity:

[source,bash]
----
$ MY_GCLOUD_USER=$(gcloud info \
  | grep Account \
  | awk -F'[][]' '{print $2}')
----

and then create a `ClusterRoleBinding`:

[source,bash]
----
$ kubectl create clusterrolebinding \
  <cluster-role-binding-name> \
  --clusterrole=cluster-admin \
  --user=${MY_GCLOUD_USER}
----

One must replace `<cluster-role-binding-name>` above with a
unique, meaningful name. It is a good idea to include one's identity in the
name (e.g., `j.doe-cluster-admin`).

=== Setup Google Cloud KMS

As the setup process generates and deals with sensitive information one will use
Google Cloud KMS to add an extra layer of security to the process. To do that,
one should first create a `vault` Google Cloud KMS
https://cloud.google.com/kms/docs/object-hierarchy#key_ring[keyring]:

[source,bash]
----
$ gcloud kms keyrings create vault --location global
----

Then, one should create an `etcd` and an `init` Google Cloud KMS
https://cloud.google.com/kms/docs/object-hierarchy#key[keys], which will be used
to encrypt and decrypt data:

[source,bash]
----
$ gcloud kms keys create etcd --location global --keyring vault --purpose encryption
$ gcloud kms keys create init --location global --keyring vault --purpose encryption
----

[[bookmark-cfssl]]
=== Install cfssl and cfssljson

`cfssl` can be installed either

* from https://github.com/cloudflare/cfssl[source] — this requires a Go 1.6+
  environment setup on one's workstation; or
* by downloading the https://pkg.cfssl.org/[binaries] for one's architecture and dropping them on `${PATH}`.

If downloading the binaries one needs only to download `cfssl` and `cfssljson`.

[[bookmark-jq]]
=== Install `jq`

`jq` can be installed either

* by downloading the appropriate binary from
  https://stedolan.github.io/jq/[the project's page] and dropping it on
  `${PATH}`; or
* by executing `brew install jq` if on macOS.

=== Install Vault client

Vault client can be installed by downloading the
https://www.vaultproject.io/downloads.html[binary] for one's architecture and
dropping it on `${PATH}`. The binary can act both as server and client, and
will be needed later on to configure the Vault deployment.

== Creating the `etcd` and `vault` namespaces

To better group and manage the major components of the solution it is
recommended to create two namespaces in the cluster — one for etcd-related
resources and another one for Vault-related ones:

[source,bash]
----
$ kubectl create namespace etcd
namespace "etcd" created
----

[source,bash]
----
$ kubectl create namespace vault
namespace "vault" created
----

[IMPORTANT]
====
If one does not follow this approach, or if one chooses different names for the
namespaces, one must update the scripts and Kubernetes descriptors in this
repository accordingly.
====

== Deploying etcd

=== Deploying `etcd-operator`

`etcd-operator` will be responsible for managing the etcd cluster that Vault
will use as storage backend. It will handle tasks such as
<<#bookmark-etcd-vault-periodic-backups,periodic backups>> and disaster recovery.. `etcd-operator` and the cluster itself will live in the
`etcd` namespace.

To start with, and since RBAC is active on the cluster, one needs to setup
adequate permissions. To do this one needs to

* create a `ClusterRole` specifying a list of permissions;
* create a dedicated `ServiceAccount` for `etcd-operator`;
* create a `CluserRoleBinding` that grants these permissions to the service
 account.

Below are the commands needed to perform the tasks described above:

[source,bash]
----
$ kubectl create -f ./etcd-operator/etcd-operator-clusterrole.yaml
clusterrole "etcd-operator" created
----

[source,bash]
----
$ kubectl create -f ./etcd-operator/etcd-operator-serviceaccount.yaml
serviceaccount "etcd-operator" created
----

[source,bash]
----
$ kubectl create -f ./etcd-operator/etcd-operator-clusterrolebinding.yaml
clusterrolebinding "etcd-operator" created
----

One is now ready to deploy `etcd-operator` itself:

[source,bash]
----
$ kubectl create -f ./etcd-operator/etcd-operator-deployment.yaml
deployment "etcd-operator" created
----

At this point it is a good idea to check whether the deployment succeeded. One
should wait for a few seconds and then run:

[source,bash]
----
$ ETCD_OPERATOR_POD_NAME=$(kubectl get pod --namespace etcd \
  | grep etcd-operator \
  | awk 'NR==1' \
  | awk '{print $1}')
----

[source,bash]
----
$ kubectl logs --follow --namespace etcd "${ETCD_OPERATOR_POD_NAME}"
time="2017-09-26T10:48:43Z" level=info msg="etcd-operator Version: 0.6.0"
time="2017-09-26T10:48:43Z" level=info msg="Git SHA: a04c308"
time="2017-09-26T10:48:43Z" level=info msg="Go Version: go1.9"
time="2017-09-26T10:48:43Z" level=info msg="Go OS/Arch: linux/amd64"
time="2017-09-26T10:48:43Z" level=info msg="Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"etcd", Name:"etcd-operator", UID:"43a0bc38-a2a8-11e7-9f73-42010a9a0fc9", APIVersion:"v1", ResourceVersion:"2111", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' etcd-operator-3796541610-99ptj became leader"
----

If the output doesn't differ much from the example, and in particular if one
doesn't see any error messages, it is safe to proceed.

=== Securing etcd cluster

One is now almost ready to create the etcd cluster that will back the Vault
deployment. However, before proceeding, one needs to generate TLS certificates
to secure communications within and to the etcd cluster.

[NOTE]
====
Even though the etcd cluster won't be exposed to outside of the Kubernetes
cluster, and even though Vault encrypts all data before it reaches the
network, it is highly recommended to adopt additional security measures, such as enabling TLS authentication and communication within the cluster, i.e. cluster membership, and with clients of the cluster.
====

One will need different types of certificates for establishing TLS:

* A server certificate which etcd will use for serving client-to-server
 requests (such as a request for a key).
* A server certificate which etcd will use for serving server-to-server _aka_ peer-to-peer
 requests (such as clustering operations).
* A client certificate to authenticate requests from `etcd-operator`.
* A client certificate to authenticate requests from Vault.

One will also need a _Certificate Authority_ (CA) to sign these certificates.
Since one will be securing communications in cluster-internal domains (such as
`etcd-0000.etcd-vault.etcd.svc.cluster.local`) one cannot rely on an external
CA to provide these certificates. Therefore, one must bootstrap their own CA
and use it to sign these certificates.

[NOTE]
====
Since `etcd-operator` has some strict requirements on the format of the
input for TLS configuration, and due to the amount of certificates one needs
to generate, a helper script is provided at `tls/create-etcd-certs.sh`.
Running it will bootstrap the CA and sign all the necessary certificates.
====

To generate the certificates run:

[source,bash]
----
$ ./tls/create-etcd-certs.sh
2017/09/18 13:05:34 [INFO] generating a new CA key and certificate from CSR
(...)
----

[[bookmark-tls-certs]]

[IMPORTANT]
====
This will generate some `\*-crt.pem.kms` and `\*-key.pem.kms` files that will be
placed in the `tls/certs` folder. These files are encrypted using Google Cloud
KMS and may only be decrypted by an individual with the
_Cloud KMS CryptoKey Encrypter/Decrypter_ permissions on the current GCP
project. Nonetheless, one should make sure that these files are distributed only
among trusted individuals.
====

[NOTE]
====
The Certificate Authority generated in this step *_is not_* the same thing as
the Certificate Authority one is seeking to establish as a result of deploying this
project. Its only purpose is to establish trust in this particular setup of
etcd and Vault, and it *_must not_* be used for anything else.
====

As mentioned above, `etcd-operator` has strict requirements regarding the names
of the certificate files used to establish TLS communications. In particular,
`etcd-operator` expects three Kubernetes secrets to be provided when creating a
new etcd cluster:

[cols="2*"]
|===
^| Secret name
^| Description

^| `etcd-peer-tls`
<| a secret containing a certificate bundle for
 _server-to-server_ communication.
^| `etcd-server-tls`
<| a secret containing a certificate bundle for
 _client-to-server_ communication.
^| `etcd-operator-tls`
<| a secret containing a certificate bundle for
 authenticating `etcd-operator` requests.
|===

[NOTE]
====
The structure of each secret is discussed in detail in the `etcd-operator`
https://github.com/coreos/etcd-operator/blob/v0.6.0/doc/user/cluster_tls.md[docs].
In order to ease the creation of these secrets, a
helper script is provided at `tls/create-etcd-secrets.sh`. Running it will
create all the necessary secrets in the Kubernetes cluster.
====

To create the aforementioned secrets, one must run:

[source,bash]
----
$ ./tls/create-etcd-secrets.sh
secret "etcd-peer-tls" created
secret "etcd-server-tls" created
secret "etcd-operator-tls" created
secret "vault-etcd-tls" created
----

The `vault-etcd-tls` secret will be needed later on.

[NOTE]
====
At this point one should give <<bookmark-tls-certs,this note>> a second read
and decide what to do with the files in `tls/certs`, as they won't be needed for
the remainder of the procedure.
====

=== Deploying the `etcd-vault` cluster

Now that `etcd-operator` and the necessary Kubernetes secrets are adequately
setup, it is time to create the etcd cluster. To do that, one must run:

[source,bash]
----
$ kubectl create -f etcd/vault-etcd-etcdcluster.yaml
etcdcluster "etcd-vault" created
----

By default ./etcd/vault-etcd-etcdcluster.yaml[the cluster specification] is:

* Cluster name is `etcd-vault`.
* Use etcd *v3.1.10*, since `etcd-operator` doesn't support v3.2 yet.
* Have *three nodes*.
* *Hourly* backups of the data.
* Keep at most *twelve backups*. These are stored in a 2GB
 https://cloud.google.com/compute/docs/disks/[Persistent Disk].

Before proceeding any further, one must check whether the etcd cluster deployment suceeded by inspecting
pods in the `etcd` namespace:

[source,bash]
----
$ kubectl get pod --namespace etcd
NAME                                        READY     STATUS    RESTARTS   AGE
etcd-operator-3160827283-2v367              1/1       Running   0          2m
etcd-vault-0000                             1/1       Running   0          47s
etcd-vault-0001                             1/1       Running   0          39s
etcd-vault-0002                             1/1       Running   0          21s
etcd-vault-backup-sidecar-899031248-85xck   1/1       Running   0          47s
----

If one's output is similar to this it is safe to proceed.

== Deploying Vault

Vault's deployment has to be split in three parts:

* One first creates the Vault `StatefulSet` itself, which creates two Vault
  instances that are _uninitialized_ and _sealed_. This means they will not
  accept any requests except for the ones required for the initial
  configuration process.
* One then proceeds to _initializing_ the Vault storage backend and _unsealing_
  the two Vault instances. This will leave Vault in a state in which it can
  accept requests.
* One finally exposes the Vault deployment to outside the Kubernetes cluster
  and secures the deployment.

=== Initial deployment

Vault's deployment is composed of seven files:

[cols="2*"]
|===

^|File
^|Description

^|`nginx-configmap.yaml`
<|contains Nginx's configuration file
^|`vault-configmap.yaml`
<|contains Vault's
  https://www.vaultproject.io/docs/configuration/index.html[configuration]
  file
^|`vault-serviceaccount.yaml`
<|creates a service account for Vault
^|`vault-service.yaml`
<|exposes Vault as a service inside the Kubernetes cluster (both for API
  requests and clustering)
^|`vault-statefulset.yaml`
<|describes the deployment of Vault itself
^|`vault-api-service.yaml`
<|creates a `NodePort` service that exposes the Vault API
^|`vault-api-ingress.yaml`
<|exposes the Vault API to outside the Kubernetes cluster

|===

[NOTE]
====
Creating a dedicated service account for Vault doesn't bring any immediate
benefit. However, it allows us to follow the principle of least-privilege from
an early stage and to prevent some known issues with `default` service
accounts.
====

[NOTE]
====
The *_headless service_* service defined in `vault-service.yaml` supports
both the `StatefulSet` defined in `vault-statefulset.yaml` as well as
clustering and high-availability of the Vault deployment.
====

[NOTE]
====
One must create the `vault-api-service.yaml` service to support the ingress
resource in GCP, since the GCE ingress controller requires a service of type
`NodePort` to be created.
====

In this first part one will be creating the first five resources, leaving the
second service and the ingress resources for later. In order to start the
deployment one needs to run the following commands:

[IMPORTANT]
====
Before running the following commands one should update the
`vault/vault-configmap.yaml` file with the address where Vault will be made
publicly accessible (check <<#bookmark-vault-address,below>>).
====

[source,bash]
----
$ kubectl create -f vault/nginx-configmap.yaml
configmap "vault" created
----

[source,bash]
----
$ kubectl create -f vault/vault-configmap.yaml
configmap "vault" created
----

[source,bash]
----
$ kubectl create -f vault/vault-serviceaccount.yaml
serviceaccount "vault" created
----

[source,bash]
----
$ kubectl create -f vault/vault-service.yaml
service "vault" created
----

[source,bash]
----
$ kubectl create -f vault/vault-statefulset.yaml
statefulset "vault" created
----

As mentioned above, this will create two Vault instances that are
_uninitialized_ and _sealed_. This means that they will not accept requests
except for the ones required for the initial configuration process.

Before proceeding any further, one must check whether the Vault deployment suceeded by inspecting
pods in the `vault` namespace:

[source,bash]
----
$ kubectl get pod --namespace vault
NAME      READY     STATUS    RESTARTS   AGE
vault-0   1/2       Running   0          30s
vault-1   1/2       Running   0          30s
----

If one’s output is similar to this it is safe to proceed.

[IMPORTANT]
====
At this point, Vault is yet to be initialized and unsealed.
Only after it is, will Kubernetes detect the Vault service is ready to be served.
====

If one inspects the logs of a Vault container, say `vault-0`, one will find the
following output:

[source,bash]
----
$ kubectl logs --namespace vault --container vault vault-0
==> Vault server configuration:

                     Cgo: disabled
         Cluster Address: https://vault:8201
              Listener 1: tcp (addr: "0.0.0.0:8200", cluster address: "0.0.0.0:8201", tls: "disabled")
               Log Level: info
                   Mlock: supported: true, enabled: true
        Redirect Address: https://vault.example.com
                 Storage: etcd (HA available)
                 Version: Vault v0.8.3
             Version Sha: 6b29fb2b7f70ed538ee2b3c057335d706b6d4e36

==> Vault server started! Log data will stream in below:

(...)
2017/09/20 15:01:55.772792 [INFO ] core: security barrier not initialized
2017/09/20 15:02:00.774962 [INFO ] core: security barrier not initialized
2017/09/20 15:02:05.770796 [INFO ] core: security barrier not initialized
2017/09/20 15:02:10.768767 [INFO ] core: security barrier not initialized
----

These `INFO` level messages indicate that Vault hasn't been initialized
yet. Vault will keep repeating these until one takes action.

=== Initializing and unsealing Vault

[IMPORTANT]
====
This procedure must be executed by a trusted individual. One will be handling
information that, if leaked, can compromise the security of the data stored by
Vault.
====

Vault must now be initialized, and both instances must be unsealed. As the
Vault pods are not accessible from outside the cluster at this time, one needs
to establish port-forwarding to one's local workstation. To do that, one should
run the following:

[source,bash]
----
$ kubectl port-forward --namespace vault vault-0 18200:8200 // <1>
Forwarding from 127.0.0.1:18200 -> 8200
Forwarding from [::1]:18200 -> 8200
----
<1> Forwards port `8200` of the first Vault pod to the local `18200` port.

Now, one should leave this command running, open a second terminal window and:

Set the value of the `VAULT_ADDR` environment variable to the address where the first Vault pod is exposed locally.

[source,bash]
----
$ export VAULT_ADDR="http://127.0.0.1:18200" // <1>
----

Initialize Vault, encrypting the resulting information using the abovementioned
key:

[source,bash]
----
$ vault init | gcloud kms encrypt \
    --plaintext-file - \
    --ciphertext-file vault-init.kms \
    --keyring vault \
    --key init \
    --location global
----

Before proceeding one may want to check that the initialization and encryption
process were successful. To do that one must run:

[source,bash]
----
$ gcloud kms decrypt \
    --plaintext-file - \
    --ciphertext-file vault-init.kms \
    --keyring vault \
    --key init \
    --location global
Unseal Key 1: +G8hVWrVaOnEQquasRyWdE2RAFuCQumodY6YgzfJzGOD
Unseal Key 2: XpfepkWVkMWLMJRyranNQDSofE1TjXTJho+ImaozyQ6X
Unseal Key 3: wfFvslot+7s0ainbE40iIhfSk7L6rs+4prc0pjQzvxtJ
Unseal Key 4: BhWFOwkg2QTW5DkBfzZWTygWAQ3IA6pMGtUF1i+wUxOr
Unseal Key 5: iLGQSSJhBqe65zpkliOATGcCe+7d2L0wn5Nl3KO3PZW9
Initial Root Token: c689c370-22ec-8268-0ea8-4cbb50c2e00c

Vault initialized with 5 keys and a key threshold of 3. Please
securely distribute the above keys. When the vault is re-sealed,
restarted, or stopped, you must provide at least 3 of these keys
to unseal it again.

Vault does not store the master key. Without at least 3 keys,
your vault will remain permanently sealed.
----

As one may see, this outputs the five unseal keys and the initial root token for
the Vault instance. At this point it is of extreme importance to:

* establish adequate access to Google Cloud KMS in the project so that only
  trusted individuals are able to decrypt `vault-init.kms`.
* distribute `vault-init.kms` among these trusted individuals.

[NOTE]
====
Every individual with _Cloud KMS CryptoKey Encrypter/Decrypter_ permissions on
the project and access to `vault-init.kms` is able to unseal Vault and perform
operations as root.
====

Now that Vault is initialized it is time to unseal it so that it can be used. Per default configuration, one will need to unseal 3 times, each one with one non-repeated unseal key generated above.

Using the same terminal window where one ran `vault init`, one must run:

[source,bash]
----
$ for i in {1..3}; do \
    vault unseal "$(
      gcloud kms decrypt \
        --plaintext-file - \
        --ciphertext-file vault-init.kms \
        --keyring vault \
        --key init \
        --location global \
        | awk "NR==${i}" \
        | awk -F ": " '{print $2}'
      )"
done
----

This will decrypt `vault-init.kms` in-memory, pick the first three unseal keys
and perform an unseal step with each one.

The first Vault pod is now unsealed and ready to serve requests.

Inspecting pods in the `vault` namespace should now output something similar to:

[source,bash]
----
$ kubectl get --namespace vault pod
NAME      READY     STATUS    RESTARTS   AGE
vault-0   2/2       Running   0          7m
vault-1   1/2       Running   0          7m
----

Now, one must also _unseal_ the second Vault instance. One should get back to
the *_first terminal window_* — where `kubectl port-forward` is running — and
stop the running process (using `Ctrl-C`). Then, one should run

[source,bash]
----
$ kubectl port-forward --namespace vault vault-1 28200:8200 // <1>
Forwarding from 127.0.0.1:28200 -> 8200
Forwarding from [::1]:28200 -> 8200
----
<1> Forwards port `8200` of the first Vault pod to the local `28200` port.

Now one should get back to the *_second terminal window_* — where
`vault init` and `vault unseal` were run before — and repeat the instructions followed to unseal the first Vault pod.

The second Vault pod is now unsealed and ready to serve requests.

[IMPORTANT]
====
The second Vault pod will operate as a
*_standby_* instance. This means it will accept requests and forward them to the
cluster leader (which, in this case, is `vault-0`). To learn more about
clustering and high-availability in Vault one should head over to
https://www.vaultproject.io/docs/concepts/ha.html[Vault's HA documentation].

However, and for network optimization, Kubernetes will mark _standby_ instances as not ready so that any requests are directed only to the _active_ instance.
====

If one inspects the logs of the `vault-1` pod one will see something similar to:

[source,bash]
----
$ kubectl logs --container vault --namespace vault vault-1
(...)
2017/09/20 15:17:23.263728 [INFO ] core: vault is unsealed
2017/09/20 15:17:23.263728 [INFO ] core: entering standby mode
----

=== Revoke root token

There is one last step one should do before proceeding. We need to _revoke_ the
initial root token. While this may seem counter-intuitive it is in fact a
recommended practice. In the same terminal window where one ran the last
`vault unseal` command, one should run:

[source,bash]
----
$ vault auth "$(
    gcloud kms decrypt \
      --plaintext-file - \
      --ciphertext-file vault-init.kms \
      --keyring vault \
      --key init \
      --location global \
      | awk "NR==6" \
      | awk -F ": " '{print $2}'
    )"
Successfully authenticated! You are now logged in.
token: <1>
token_duration: 0
token_policies: [root]
----

This will decrypt `vault-init.kms` in-memory, pick the initial root token and
perform an authentication step.

[source,bash]
----
$ vault token-revoke -self <1>
Success! Token revoked if it existed.
----
<1> This corresponds to the _initial root token_.

The Vault deployment is now initialized, both instances are unsealed, and the
initial root token has been revoked. It is now time to continue the deployment
by exposing the Vault deployment to outside the Kubernetes cluster.

[TIP]
====
One may now return to the terminal window where `kubectl port-forward` is
running and terminate the process using Ctrl-C.
====

[IMPORTANT]
====
The *_unseal_* procedure must be performed to every new Vault pod, i.e. when a pod crashes or is restarted.
====

=== Exposing Vault

One will now expose Vault to outside the cluster, so that applications running
in other clusters can access it. To do this one needs to create a global static
IP in GCP:

[source,bash]
----
$ gcloud compute addresses create vault --global
Created [https://www.googleapis.com/compute/v1/projects/<project-name>/global/addresses/vault].
----

[source,bash]
----
$ gcloud compute addresses describe vault --global
address: <1>
creationTimestamp: '2017-09-18T05:12:33.928-07:00'
description: ''
id: '7579662126224115422'
ipVersion: IPV4
kind: compute#address
name: vault
selfLink: https://www.googleapis.com/compute/v1/projects/<project-name>/global/addresses/vault
status: RESERVED
----
<1> The IP address one will use to expose Vault.

[IMPORTANT]
====
If one creates the IP address with a different name one must update the
`vault/vault-api-ingress.yaml` file accordingly.
====

[[bookmark-vault-address]]

After the `vault` IP address is created, one must configure the DNS of the
domain one is going to use to expose Vault. For instance, if one wants to
expose Vault at `https://vault.example.com` one has to create a DNS record with
type `A` and name `vault` pointing to the abovementioned IP address at the DNS
provider for the `example.com` domain. The steps to do this are highly
dependent on the DNS provider for the domain and cannot be detailed here.

From this point on, it is assumed that DNS has been properly configured
and that changes have propagated. One can test whether changes have propagated
by using `dig`:

[source,bash]
----
dig @8.8.8.8 vault.example.com A

; <<>> DiG 9.8.3-P1 <<>> @8.8.8.8 vault.example.com A
; (1 server found)
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 43874
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0

;; QUESTION SECTION:
;vault.example.com.		IN	A

;; ANSWER SECTION:
vault.example.com.	299	IN	A	<2>

;; Query time: 61 msec
;; SERVER: 8.8.8.8#53(8.8.8.8)
;; WHEN: Mon Sep 18 13:13:49 2017
;; MSG SIZE  rcvd: 53
----
<1> Looks-up `A` records for `vault.example.com` at Google Public DNS (`8.8.8.8`).
<2> This must match the global IP address created above.

[IMPORTANT]
====
It is highly recommended to wait for changes to propagate before proceeding.
====

[IMPORTANT]
====
Before running the following commands, one should update the
`vault/vault-api-ingress.yaml` file with the actual domain name used to expose
Vault.
====

Once the `vault` IP address is created, one must create the service and ingress
resources:

[source,bash]
----
$ kubectl create -f vault/vault-api-service.yaml
service "vault" created
----

[source,bash]
----
$ kubectl create -f vault/vault-api-ingress.yaml
ingress "vault" created
----

The above will create a
https://cloud.google.com/compute/docs/load-balancing/[_global *external* load-balancer_]
pointing to the Vault deployment.

In order to secure Vault external access one must now configure HTTPS access.
The easiest and cheapest way to obtain a trusted TLS certicate is using
https://letsencrypt.org/[Let's Encrypt], and the easiest way to automate the
process of obtaining and renewing certificates from Let's Encrypt is by using
https://github.com/jetstack/kube-lego[`kube-lego`]:

[IMPORTANT]
====
Before running the following commands one should update the
`kube-lego/kube-lego.yaml` file with one's information, i.e. email.
====

[source,bash]
----
$ kubectl create -f ./kube-lego/kube-lego-bundle.yaml
namespace "kube-lego" created
configmap "kube-lego" created
deployment "kube-lego" created
----

As soon as it starts, `kube-lego` will start monitoring ingress resources and
requesting certificates from Let's Encrypt. One can check that the deployment
succeeded by running the following:

[source,bash]
----
$ KUBE_LEGO_POD_NAME=$(kubectl get --namespace kube-lego pod \
  | grep kube-lego \
  | awk 'NR==1' \
  | awk '{print $1}')
----

[source,bash]
----
$ kubectl logs --namespace kube-lego "${KUBE_LEGO_POD_NAME}"
time="2017-09-18T12:16:06Z" level=info msg="kube-lego 0.1.5-a9592932 starting" context=kubelego
time="2017-09-18T12:16:06Z" level=info msg="connecting to kubernetes api: https://10.15.240.1:443" context=kubelego
time="2017-09-18T12:16:06Z" level=info msg="successfully connected to kubernetes api v1.7.5" context=kubelego
time="2017-09-18T12:16:06Z" level=info msg="server listening on http://:8080/" context=acme
(...)
----

[IMPORTANT]
====
Let's Encrypt must be able to reach port `TCP 80` on domains for which certificates
are requested, so one must use the

[source,yaml]
----
kubernetes.io/ingress.allow-http: "true"
----

annotation in `vault/vault-api-ingress.yaml`. Please note that it is safe to
set the abovementioned annotation, since the NGINX instance that is deployed
alongside Vault makes sure that Vault only communicates over HTTPS.
Any request to Vault via plain HTTP will be rejected.
====

If everything goes well, after a short while one will be able to access

https://vault.example.com/v1/sys/health

securely. On the other hand, one will not be able to access
http://vault.example.com/v1/sys/health.
